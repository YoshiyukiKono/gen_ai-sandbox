{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc6c7cb-0245-46a2-aabc-3dd9f1b4b086",
   "metadata": {
    "id": "0cc6c7cb-0245-46a2-aabc-3dd9f1b4b086"
   },
   "source": [
    "# Homework: Implementing a RAG Example with Ollama and Mistral LLM\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will be working on a practical application of the DataStax RAGStack. The goal is to modify this Jupyter Notebook that currently leverages OpenAI's LLM (Large Language Models) for a RAG example. Your task is to adapt this notebook to use Ollama running Mistral LLM, as the backbone for the RAG implementation.\n",
    "\n",
    "## Why Ollama?\n",
    "\n",
    "Ollama offers the option to run a LLM on a local machine. Self-managed LLMs are especially of interest for Customers using Cassandra or DSE on-prem and in internet-restricted environments, and for those using Cassandra, DSE and Astra DB who are cautious about sending sensitive data to cloud-based LLM services due to privacy concerns and cost considerations. Ollama enables local execution of Large Language Models, providing a private solution. This is particularly beneficial for demonstrations and aligns with the requirements of businesses handling critical data, ensuring it remains within their controlled environment.\n",
    "\n",
    "For those seeking self-managed LLMs, alternatives like Mistral are available, offering performance comparable to OpenAI's models. Interested parties are encouraged to [review the Mistral documentation](https://huggingface.co/docs/transformers/main/en/model_doc/mistral). Mistral is designed for easy installation and can be efficiently hosted on the robust computing resources available in customer data centers.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Understand the Current Implementation**: Begin by familiarizing yourself with the existing Jupyter Notebook. It uses DataStax's RAGStack, integrating Astra DB as a vector store, and employs an OpenAI LLM for generating responses.\n",
    "\n",
    "2. **Transition to Ollama and Mistral LLM**: Your primary task is to modify the code in the notebook to replace the OpenAI LLM with Ollama running Mistral LLM. This will involve understanding the differences between the two models and adapting the API calls and data handling accordingly.\n",
    "\n",
    "3. **Test and Validate**: Keep in mind you run notebook and ollama on your local machine. After implementing the changes, test the notebook to ensure that it functions correctly with the new LLM.\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **Ollama Documentation**: [How to install Ollama](https://ollama.com/download) and [How to run Mistral LLM powered by Ollama](https://ollama.com/library/mistral/tags)\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "- Complete the task in the provided Jupyter Notebook.\n",
    "- Ensure all code cells are well-documented.\n",
    "- Submit the final notebook along with a brief report summarizing your approach, key challenges, and solutions.\n",
    "\n",
    "Good luck, and feel free to reach out if you have any questions or need further clarifications!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e5e9d7-37bd-474b-9c75-f3a82daf41c0",
   "metadata": {
    "id": "d4e5e9d7-37bd-474b-9c75-f3a82daf41c0",
    "outputId": "63205edf-6459-45b0-e52c-1d2ed8edaf35",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragstack-ai in /opt/conda/lib/python3.11/site-packages (0.9.0)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: astrapy<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from ragstack-ai) (0.7.7)\n",
      "Requirement already satisfied: cassio<0.2.0,>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ragstack-ai) (0.1.5)\n",
      "Requirement already satisfied: langchain==0.1.12 in /opt/conda/lib/python3.11/site-packages (from ragstack-ai) (0.1.12)\n",
      "Requirement already satisfied: langchain-community==0.0.28 in /opt/conda/lib/python3.11/site-packages (from ragstack-ai) (0.0.28)\n",
      "Requirement already satisfied: langchain-core==0.1.31 in /opt/conda/lib/python3.11/site-packages (from ragstack-ai) (0.1.31)\n",
      "Requirement already satisfied: langchain-openai==0.0.8 in /opt/conda/lib/python3.11/site-packages (from ragstack-ai) (0.0.8)\n",
      "Requirement already satisfied: llama-index==0.9.48 in /opt/conda/lib/python3.11/site-packages (from llama-index[langchain]==0.9.48->ragstack-ai) (0.9.48)\n",
      "Requirement already satisfied: llama-parse==0.1.4 in /opt/conda/lib/python3.11/site-packages (from ragstack-ai) (0.1.4)\n",
      "Requirement already satisfied: unstructured==0.12.5 in /opt/conda/lib/python3.11/site-packages (from ragstack-ai) (0.12.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (1.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (0.1.25)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.12->ragstack-ai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.11/site-packages (from langchain-core==0.1.31->ragstack-ai) (4.0.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core==0.1.31->ragstack-ai) (23.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from langchain-openai==0.0.8->ragstack-ai) (1.14.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /opt/conda/lib/python3.11/site-packages (from langchain-openai==0.0.8->ragstack-ai) (0.6.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (2024.2.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (1.5.8)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (3.8.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (0.9.0)\n",
      "Requirement already satisfied: chardet in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (5.2.0)\n",
      "Requirement already satisfied: filetype in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (0.4.27)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (5.1.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (4.12.2)\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (2.10.1)\n",
      "Requirement already satisfied: python-iso639 in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (2024.2.7)\n",
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (3.6.2)\n",
      "Requirement already satisfied: backoff in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (2.2.1)\n",
      "Requirement already satisfied: unstructured-client>=0.15.1 in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (0.22.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from unstructured==0.12.5->ragstack-ai) (1.16.0)\n",
      "Collecting transformers<5.0.0,>=4.32.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.66.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.2.1-cp311-cp311-manylinux2014_aarch64.whl.metadata (25 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.4.1.post1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.15.1 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-10.2.0-cp311-cp311-manylinux_2_28_aarch64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: deprecation<2.2.0,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from astrapy<0.8.0,>=0.7.0->ragstack-ai) (2.1.0)\n",
      "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /opt/conda/lib/python3.11/site-packages (from astrapy<0.8.0,>=0.7.0->ragstack-ai) (0.10.2)\n",
      "Requirement already satisfied: cassandra-driver>=3.28.0 in /opt/conda/lib/python3.11/site-packages (from cassio<0.2.0,>=0.1.3->ragstack-ai) (3.29.0)\n",
      "Collecting filelock (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.32.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.32.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->ragstack-ai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->ragstack-ai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->ragstack-ai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->ragstack-ai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.12->ragstack-ai) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core==0.1.31->ragstack-ai) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core==0.1.31->ragstack-ai) (1.3.0)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in /opt/conda/lib/python3.11/site-packages (from cassandra-driver>=3.28.0->cassio<0.2.0,>=0.1.3->ragstack-ai) (0.2.1.post1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.12->ragstack-ai) (3.21.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /opt/conda/lib/python3.11/site-packages (from httpx[http2]<1,>=0.25.2->astrapy<0.8.0,>=0.7.0->ragstack-ai) (4.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.12->ragstack-ai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.12->ragstack-ai) (3.9.15)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (8.1.7)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.0.8->ragstack-ai) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.1.12->ragstack-ai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.1.12->ragstack-ai) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.12->ragstack-ai) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.12->ragstack-ai) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.12->ragstack-ai) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (1.0.0)\n",
      "Requirement already satisfied: deepdiff>=6.0 in /opt/conda/lib/python3.11/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.5->ragstack-ai) (6.7.1)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in /opt/conda/lib/python3.11/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.5->ragstack-ai) (1.0.6)\n",
      "Requirement already satisfied: pypdf>=4.0 in /opt/conda/lib/python3.11/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.5->ragstack-ai) (4.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.5->ragstack-ai) (2.8.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/conda/lib/python3.11/site-packages (from unstructured-client>=0.15.1->unstructured==0.12.5->ragstack-ai) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->unstructured==0.12.5->ragstack-ai) (2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index==0.9.48->llama-index[langchain]==0.9.48->ragstack-ai) (2024.1)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /opt/conda/lib/python3.11/site-packages (from deepdiff>=6.0->unstructured-client>=0.15.1->unstructured==0.12.5->ragstack-ai) (4.1.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /opt/conda/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<0.8.0,>=0.7.0->ragstack-ai) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /opt/conda/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<0.8.0,>=0.7.0->ragstack-ai) (4.0.0)\n",
      "Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.1-cp311-cp311-manylinux2014_aarch64.whl (86.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp311-cp311-manylinux_2_28_aarch64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.4.1.post1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (34.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, Pillow, filelock, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed Pillow-10.2.0 filelock-3.13.1 huggingface-hub-0.21.4 mpmath-1.3.0 safetensors-0.4.2 scikit-learn-1.4.1.post1 scipy-1.12.0 sentence-transformers-2.5.1 sympy-1.12 threadpoolctl-3.3.0 tokenizers-0.15.2 torch-2.2.1 transformers-4.38.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ragstack-ai sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2fe127-ddca-470c-9692-7b2a218d0e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Astra DB API Endpoint: https://1014346a-a40c-4d1a-b1a3-78769cc72312-us-east1.apps.astra.datastax.com\n"
     ]
    }
   ],
   "source": [
    "ASTRA_DB_API_ENDPOINT = input(\"Astra DB API Endpoint:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f99fe7-000b-4dcb-a89b-b4e784daed0f",
   "metadata": {
    "id": "a4f99fe7-000b-4dcb-a89b-b4e784daed0f",
    "outputId": "92e16af8-5ab8-4868-c1a3-134503a2dff2"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your ASTRA_DB_APPLICATION_TOKEN:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "ASTRA_DB_APPLICATION_TOKEN = getpass.getpass(\"Enter your ASTRA_DB_APPLICATION_TOKEN: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4d0098-515e-4a3e-a0fb-c8eae568786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Astra DB: {'status': {'collections': ['book_collection', 'langchain_message_store', 'pdf_collection', 'vector_context_datastax', 'vector_test']}}\n"
     ]
    }
   ],
   "source": [
    "from astrapy.db import AstraDB\n",
    "\n",
    "db = AstraDB(\n",
    "  token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "  api_endpoint=ASTRA_DB_API_ENDPOINT)\n",
    "\n",
    "print(f\"Connected to Astra DB: {db.get_collections()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e3ddd0-5078-4398-8ef2-3465b7770dcd",
   "metadata": {
    "id": "b8e3ddd0-5078-4398-8ef2-3465b7770dcd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61622a71-de5f-44e7-bc06-ae4555530304",
   "metadata": {
    "id": "61622a71-de5f-44e7-bc06-ae4555530304"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "text = response.text\n",
    "\n",
    "f = open('essay.txt', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "079ea2c2-b256-4942-aec8-cbf6b69d5697",
   "metadata": {
    "id": "079ea2c2-b256-4942-aec8-cbf6b69d5697"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import AstraDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf4a4160-6b69-4759-96b6-83d97d2efde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "ollama = Ollama(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82faa252-77ed-4fb0-a5db-3161f4cfd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.base_url = 'http://host.docker.internal:11434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da9a498-9202-49a9-8292-cf6373d828e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The color of the sky appears blue due to a process called Rayleigh scattering. When the sun's rays enter Earth's atmosphere, they are scattered in all directions by gas molecules and particles present in the air. Blue light has a shorter wavelength and gets scattered more easily than other colors such as red or yellow. As a result, when we look up at the sky, we predominantly see the blue light that has been scattered, making the sky appear blue to our eyes.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.invoke(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb2b6310-7b61-4cd1-b329-b8e52275cca5",
   "metadata": {
    "id": "bb2b6310-7b61-4cd1-b329-b8e52275cca5",
    "outputId": "e9d66cb5-4f1b-43d6-b9cd-386491e45763"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = TextLoader(\"essay.txt\")\n",
    "docs = loader.load()\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "# Define the embedding model\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "#vector = load_vector_store()\n",
    "vector = AstraDB.from_documents(documents, embeddings,collection_name=\"essay_for_ollama\", api_endpoint=ASTRA_DB_API_ENDPOINT, token=ASTRA_DB_APPLICATION_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b12f551a-19b0-4b7c-be9f-510e4da46b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The two main things the author worked on before college were Interleaf, a software company where he did Lisp programming, and freelance Lisp programming for other clients. He also wrote a Lisp book during this time.\n"
     ]
    }
   ],
   "source": [
    "# Define a retriever interface\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# Define prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# Create a retrieval chain to answer questions\n",
    "document_chain = create_stuff_documents_chain(ollama, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "response = retrieval_chain.invoke({\"input\": \"What were the two main things the author worked on before college?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3ac18bf-71a2-4bb0-b518-16ef859ac7b8",
   "metadata": {
    "id": "a3ac18bf-71a2-4bb0-b518-16ef859ac7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author worked mainly on two things before college: writing and programming. He wrote short stories but they were awful, with hardly any plot. The first programs he tried writing were on an IBM 1401 computer using a punch card system. He couldn't figure out what to do with it and his clearest memory is of the moment he learned it was possible for programs not to terminate, which was both a technical and social error. With microcomputers, everything changed as you could have a computer sitting right in front of you that could respond to your keystrokes as it ran instead of just churning through a stack of punch cards and then stopping. The first friend of the author's to get a microcomputer built it himself using a kit called Heathkit. Computers were expensive in those days and it took the author years of nagging before his father finally bought one, a TRS-80, around 1980. From then on he really started programming, writing simple games, programs to predict how high his model rockets would fly, and even a word processor that his father used to write at least one book.\n",
      "\n",
      "[context]\n",
      "\n",
      "Before college the two main things I worked on outside of school were writing and programming. I didn’t write essays. I wrote what beginning writers were supposed to write then: short stories. My stories were awful, they had hardly any plot, just characters with strong feelings, which I imagined made them deep. The first programs I tried writing were on the IBM 1401 that our school district used for data processing. This was in 9th grade, so I was 13 or 14. The school district’s 1401 happened to be in the basement of our junior high school, and Rich Draves and I got permission to use it. It looked like a mini Bond villain’s lair down there: all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
      "\n",
      "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would usually be printing something loud on the printer.\n",
      "\n",
      "I couldn’t figure out what to do with the 1401. And in retrospect there wasn't much I could have done with it. The only form of input programs had was data stored on punch cards, and I didn't have any data stored on punch cards. The only other option was things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So it's not surprising I can't remember any programs I wrote because they couldn't have done much. My clearest memory is of the moment one of my programs didn't terminate. On a machine without time-sharing, this was both a social and technical error, as the data center manager's expression made clear.\n",
      "\n",
      "With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. The first friend of mine to get a microcomputer built it himself using a kit called Heathkit. I remember vividly how impressed and envious I felt watching him sit in front of it, typing programs right into the computer.\n",
      "\n",
      "Computers were expensive back then and it took me years of nagging before my father finally bought one, a TRS-80, around 1980. From then on I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and even a word processor that my father used to write at least one book.\n",
      "</context>\n",
      "\n",
      "回答:\n",
      "\n",
      "前Collegeで2つの大きなものを僚いたのは、書やプログラム。文章は短いストリーがあるだたりです。作品は、IBM1401で、ポンチカードを使うことある, Fortranで、結果は、印刷器に何も出力された。\n",
      "\n",
      "その機梱は，私は解決しなかった.今後見る理由は，データセンターマネジメントの面の睦いにあるデータベースから取り入ってることなくだ。プログラムのみは，偶力しないものを持てる（時間分共用シーリング）、また実数学知足だけでない。\n",
      "\n",
      "マイコンピュータあたが大変な元入り，あから1980年に買っては。からあとは、僅い遊です。私の遥いは開始した。ゲーム、プログラムに入するものを作った。高さのロケットモデルの高さを份ぐ\n",
      "</answer>\n",
      "\n",
      "There was no significant difference between what I could have done with the IBM 1401 and microcomputers. Writing involved short stories, which weren't much more than that. Programs were stored on punch cards, and Fortran was used to process them. The result was outputted through a printer.\n",
      "\n",
      "However, I couldn't figure out what to do with the IBM 1401. And in retrospect, there wasn't really anything I could have done with it. Only input for programs came from data stored on punch cards, and I didn't possess any of that. Furthermore, mathematics was not enough for me to create something interesting using mathematical knowledge.\n",
      "\n",
      "Microcomputers changed everything, however. You could put a computer right in front of you on your desk, which could respond to your keystrokes while it was running. My first friend possessed one and I remember vividly how impressed and envious I felt as I watched him sit in front of it typing programs into the computer.\n",
      "\n",
      "Computers were expensive then and it took me years of nagging before my father finally bought one, a TRS-80, around 1980. From that time on I really started programming. I made simple games, programs to predict the height of my model rockets, even a word processor that my father used to write at least one book.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"著者が大学入学前に取り組んだ主な 2 つのことは何ですか?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf39fe9-13d3-4317-9d5a-3d0036f2e6d0",
   "metadata": {},
   "source": [
    "The above Japanese output is totally useless"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
