{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPgbpy+Z43k1H86qEtClMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoshiyukiKono/gen_ai-sandbox/blob/main/langchain_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1LS7ag0jZlUxEdgCi1NG9S90cfTkCBQbH?usp=sharing#scrollTo=ydyK_bgrOHbG"
      ],
      "metadata": {
        "id": "LQe1IjldVQAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "439Jro8zUyVY",
        "outputId": "823f61d3-e7d3-4fd1-e94c-1211f850bbb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.27.9\n",
            "  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/75.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.9) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.9) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.9) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.9) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.9) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.9) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.9) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.9) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.9) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.9) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.9) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.9) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.9) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.9\n",
            "Collecting langchain==0.0.272\n",
            "  Downloading langchain-0.0.272-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.272)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (2.11.1)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.272)\n",
            "  Downloading langsmith-0.0.35-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.272) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.272) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.272) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.272) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.272) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.272) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.272) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.272)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.272)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.11.1->langchain==0.0.272) (1.60.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.11.1->langchain==0.0.272) (3.20.3)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.11.1->langchain==0.0.272) (2.17.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.272) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.272) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.272) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.272) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.272) (2.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain==0.0.272) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain==0.0.272) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain==0.0.272) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain==0.0.272) (4.9)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.272) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.272)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.11.1->langchain==0.0.272) (0.5.0)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.272 langsmith-0.0.35 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting tiktoken==0.4.0\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.27.9\n",
        "!pip install langchain==0.0.272\n",
        "!pip install tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "#TODO: APIキーの登録が必要\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "_GLvWypMVHAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.jsonl\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"Fine-tuningって何？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！Fine-tuningとは、学習済みの言語モデルを特定のタスクに最適化することです。\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"Fine-tuningのメリットを3つ教えて。\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！「参考テキストの入力の手間が省ける点」と「膨大な知識量を学習できる点」と「システムとしての応答速度が改善される点」が挙げられます。\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"Fine-tuningのデメリットを3つ教えて。\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！「学習コストがかかる点」と「利用コストが高くなる点」と「AIとしての汎用性は下がってしまう点」が挙げられます。\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainとは？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！LangChainとは「ChatGPTなどの言語系モデルの機能拡張を効率的に実装するためのライブラリ」です。\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainのメリットは何？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！LangChainを用いることで、最新情報に対応できる点や長文を入力できる点、複雑な計算問題に回答できる点が挙げられます。\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainの実装方法を教えて。\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！LangChainを使用するための環境構築やAPIキーの設定方法、さらに具体的なコード例を紹介します。まずは必要なライブラリのインストールとAPIキーの設定を行い、その後OpenAIモデルの呼び出しやプロンプトのテンプレート作成、LLM Chainの実行方法を示します。\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainの具体的な機能は何？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！LangChainは複数の具体的な機能を実装しています。具体的な機能としては、Models（言語モデルの切り替えや組み合わせ）、Prompt（プロンプトの管理や最適化）、Indexes（外部データを用いた回答生成）、Chains（複数のプロンプト入力の実行）、Agents（モデルによるアクションの決定と実行）、Memory（状態保持）などがあります。\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainのAgents機能はどのようなもの？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！Agentsは言語モデルにツールを用いて次のアクションを決定し、実行、観測、完了する機能です。例えば、与えられたツールに基づいてモデルがGoogle検索をし、Pythonコードを生成し、テストを行ってくれるイメージです。\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainのIndexes機能はどのような場面で役立つ？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！IndexesはPDFやCSVなどの外部データを用いて回答を生成する機能です。例えば、社内のPDFを利用してチャットボットを作成する場面で便利です。また、機密情報を外部サーバーに送信せずに自社システム上で効率的に実装できる点も魅力です。\"}]}\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainのMemory機能は何に役立つ？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！MemoryはChainsやAgents内部の状態保持を可能にする機能です。これにより、チャットのやり取りの状態を保持し、過去の会話を考慮した回答を生成できるようになります。従来の静的なクエリ単位の処理から進化した機能と言えるでしょう。\"}]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BREKKFLJVKlj",
        "outputId": "e7a8f1d2-3f02-4f70-8b5b-c0432105d6d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat train.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMOySXRNVZpk",
        "outputId": "10b3b079-a9c4-44ce-ba75-270e40728710"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Fine-tuningって何？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！Fine-tuningとは、学習済みの言語モデルを特定のタスクに最適化することです。\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Fine-tuningのメリットを3つ教えて。\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！「参考テキストの入力の手間が省ける点」と「膨大な知識量を学習できる点」と「システムとしての応答速度が改善される点」が挙げられます。\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"Fine-tuningのデメリットを3つ教えて。\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！「学習コストがかかる点」と「利用コストが高くなる点」と「AIとしての汎用性は下がってしまう点」が挙げられます。\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainとは？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！LangChainとは「ChatGPTなどの言語系モデルの機能拡張を効率的に実装するためのライブラリ」です。\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainのメリットは何？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！LangChainを用いることで、最新情報に対応できる点や長文を入力できる点、複雑な計算問題に回答できる点が挙げられます。\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainの実装方法を教えて。\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！LangChainを使用するための環境構築やAPIキーの設定方法、さらに具体的なコード例を紹介します。まずは必要なライブラリのインストールとAPIキーの設定を行い、その後OpenAIモデルの呼び出しやプロンプトのテンプレート作成、LLM Chainの実行方法を示します。\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainの具体的な機能は何？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！LangChainは複数の具体的な機能を実装しています。具体的な機能としては、Models（言語モデルの切り替えや組み合わせ）、Prompt（プロンプトの管理や最適化）、Indexes（外部データを用いた回答生成）、Chains（複数のプロンプト入力の実行）、Agents（モデルによるアクションの決定と実行）、Memory（状態保持）などがあります。\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainのAgents機能はどのようなもの？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！Agentsは言語モデルにツールを用いて次のアクションを決定し、実行、観測、完了する機能です。例えば、与えられたツールに基づいてモデルがGoogle検索をし、Pythonコードを生成し、テストを行ってくれるイメージです。\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainのIndexes機能はどのような場面で役立つ？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！IndexesはPDFやCSVなどの外部データを用いて回答を生成する機能です。例えば、社内のPDFを利用してチャットボットを作成する場面で便利です。また、機密情報を外部サーバーに送信せずに自社システム上で効率的に実装できる点も魅力です。\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"LangChainのMemory機能は何に役立つ？\"}, {\"role\": \"assistant\", \"content\": \"ご質問ありがとうございます！MemoryはChainsやAgents内部の状態保持を可能にする機能です。これにより、チャットのやり取りの状態を保持し、過去の会話を考慮した回答を生成できるようになります。従来の静的なクエリ単位の処理から進化した機能と言えるでしょう。\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "with open(jsonl_file_path) as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)\n",
        "\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(\n",
        "            message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")\n",
        "\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(\n",
        "    messages,\n",
        "    tokens_per_message=3,\n",
        "    tokens_per_name=1,\n",
        "):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(\n",
        "    f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\"\n",
        ")\n",
        "\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_EPOCHS = 1\n",
        "MAX_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(\n",
        "    min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(\n",
        "    f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\"\n",
        ")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(\n",
        "    f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\"\n",
        ")\n",
        "print(\"See pricing page to estimate total costs\")"
      ],
      "metadata": {
        "id": "99XJbG7nVp-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = openai.File.create(\n",
        "    file=open(jsonl_file_path, \"rb\"),\n",
        "    purpose=\"fine-tune\",\n",
        ")\n",
        "\n",
        "file_id = res[\"id\"]\n",
        "print(file_id)"
      ],
      "metadata": {
        "id": "Zk2F3AtuVyNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = openai.FineTuningJob.create(training_file=file_id, model=\"gpt-3.5-turbo\")\n",
        "ft_id = res[\"id\"]\n",
        "print(ft_id)"
      ],
      "metadata": {
        "id": "x29xy9-kV0TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最大10個のモデルを取得\n",
        "model_list = openai.FineTuningJob.list(limit=10)\n",
        "print(model_list)\n",
        "\n",
        "# # モデルの状態を取得\n",
        "# openai.FineTuningJob.retrieve(ft_id)\n",
        "\n",
        "# # 特定のモデルにおける最大10個のジョブイベントを出力\n",
        "# openai.FineTuningJob.list_events(id=ft_id, limit=10)"
      ],
      "metadata": {
        "id": "7mCQWcq0V1--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model_name = model_list[\"data\"][0][\"fine_tuned_model\"]\n",
        "print(fine_tuned_model_name)"
      ],
      "metadata": {
        "id": "xrfamndLV33R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ジョブのキャンセル\n",
        "# openai.FineTuningJob.cancel(ft_id)"
      ],
      "metadata": {
        "id": "yeZLY5DPV5er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # モデルの削除（組織のオーナーのみ実行可能）\n",
        "# openai.Model.delete(ft_id)"
      ],
      "metadata": {
        "id": "ADl-1n6rV6-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion = openai.ChatCompletion.create(\n",
        "    model=fine_tuned_model_name,\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"ChatGPTについて教えてください。\"\n",
        "    }],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message[\"content\"])"
      ],
      "metadata": {
        "id": "prW8pkWdV9Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=fine_tuned_model_name)\n",
        "res = llm.predict(\"ChatGPTについて教えてください。\")\n",
        "print(res)"
      ],
      "metadata": {
        "id": "ZPGIplXvV_Tn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}